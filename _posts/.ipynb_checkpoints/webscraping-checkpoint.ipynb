{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Guide\n",
    "- [Learning Objectives](#learning-objectives)\n",
    "- [Background](#background)\n",
    "\t- [Overview](#overview)\n",
    "- [Part 1. Web Scraping with Beautiful Soup](#web)\n",
    "\t- [Installation](#installation)\n",
    "\t- [Example: Scraping a website](#scraping)\n",
    "- [Part 2. Location Data with Google Maps API](#google)\n",
    "\t- [API Key](#key)\n",
    "\n",
    "\n",
    "\n",
    "    - [Further Study](#study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "## Learning Objectives\n",
    "\n",
    "- Recognize the types of problems web scraping is useful for.\n",
    "- Observe an application of web scraping to an example web site.\n",
    "- Determine how to apply web scraping to collect the data of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"background\"></a>\n",
    "## Background\n",
    "\n",
    "It's no secret that I have what some might call an 'unhealthy' obsession with the tracking the snow that may or may not be headed our way. For years, one of my favorite tools for this was an awesome app (that disappeared years ago) called [FreshyMap](https://en.wikipedia.org/wiki/FreshyMap).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/3/3f/FreshyMapshot.jpg\" style=\"width: 400px;\"/>\n",
    "\n",
    "{% include figure_link.html url=\"https://upload.wikimedia.org/wikipedia/en/3/3f/FreshyMapshot.jpg\" href=\"https://en.wikipedia.org/wiki/FreshyMap\" caption=\"Screenshot of a FreshyMap session Image source: wikipedia.com\" width=\"40%\" %}\n",
    "\n",
    "> So, I sez: \"Hey! What better opportunity to improve my skills with Flask, interact with some new APIs, and try out some hot new mapping platforms than to create my own homage to the late, great FreshyMap? \n",
    ">\n",
    "> #### Check out the latest version [here](https://snowscraper.herokuapp.com/).\\*  \n",
    "\\*Doesn't look great via cell-phone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "#### Overview\n",
    "This software does two main things:\n",
    "1. **Scrapes** weather data on every resort in North America (from the good folks at [Opensnow](https://www.opensnow.com)) and integrate that with data from [Google Maps](https://developers.google.com/console).\n",
    "\n",
    "1. Provides a highly interactive **visualization** tool using ([Plotly Dash](https://dash.plot.ly/)) that I've dubbed \"FreshyFinder\", to search the aggregated snowfall data in anticipation of spending your days in that perfect POW.\n",
    "\n",
    "Since each of these facets deserve a fair amount of time to unpack, I've decided to break this into two posts. In this post, we'll focus on web scraping with [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"web\"></a>\n",
    "## Part 1. Web Scraping with Beautiful Soup\n",
    "The [documentation](https://www.crummy.com/software/BeautifulSoup/) for Beautiful Soup (BS) describes it as \"a Python library designed for quick turnaround projects like screen-scraping.\"\n",
    "\n",
    "> BS parses anything you give it, and does the tree traversal stuff for you. You can tell it \n",
    "- \"Find all the links\", or \n",
    "- \"Find all the links of class externalLink\", or \n",
    "- \"Find all the links whose urls match \"foo.com\", or \n",
    "- \"Find the table heading that's got bold text, then give me that text.\"\n",
    ">\n",
    "> **Valuable data that was once locked up in poorly-designed websites is now within your reach.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"installation\"></a>\n",
    "### Installation\n",
    "1. You can install Beautiful Soup 4\\*  with `pip install beautifulsoup4`.\n",
    "2. You may also need to install the requests toolkit. If so, use `pip` for that, as well.  \n",
    "`pip install requests`\n",
    "\n",
    "\\*the latest, as of Jan '19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scraping\"></a>\n",
    "### Example: Scraping a website\n",
    "First, let's see how we can collect data from a website such as [opensnow.com](www.opensnow.com).\n",
    "> Before we begin, we want to develop a 'plan-of-attack' for how we are going to systematically collect the data we are interested in. In our case, we are interested in retrieving info on every ski resort in North America. This website happens to have a page for each state, that contains all of the info we need to then scrape from each resort in that state.\n",
    ">\n",
    "> *So*, we need to figure out how to:  \n",
    "> **A.** iteratively scrape data for *each state*, then  \n",
    "> **B.** scrape data for *each resort*.\n",
    "\n",
    "Normally, this sounds like a job that might require an ugly *nested loop*. But with BS, the process it made much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with a list of all of the states and provinces we could possibly scrape\n",
    "states = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "    'Alberta': 'AB',\n",
    "    'British Columbia': 'BC',\n",
    "    'Manitoba':  'MB',\n",
    "    'New Brunswick':  'NB',\n",
    "    'Newfoundland and Labrador':  'NL',\n",
    "    'Northwest Territories': 'NT',\n",
    "    'Nova Scotia':  'NS',\n",
    "    'Nunavut':  'NU',\n",
    "    'Ontario':  'ON',\n",
    "    'Prince Edward Island':  'PE',\n",
    "    'Quebec':  'QC',\n",
    "    'Saskatchewan':  'SK',\n",
    "    'Yukon':  'YT'\n",
    "}\n",
    "\n",
    "state_abbrevs = list(states.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we'll start with the second state in the list: **Alaska**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://opensnow.com/state/AK\n"
     ]
    }
   ],
   "source": [
    "# Create URL for the first state's website\n",
    "base = 'https://opensnow.com'\n",
    "url = base+'/state/'+state_abbrevs[1]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape state website\n",
    "resp = requests.get(url)\n",
    "state_soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to scrape the data for an individual state, we can collect the links to *each resort* in the state and iterate through each of them (in the next step), to end up with the snow forecast for each.\n",
    "\n",
    "> Now is a good time to explain the difference between `.find()` and `.find_all()`. The latter method scans the entire document looking for matching results, but often you'll only expect to find one result.\n",
    ">\n",
    "> There's a great example in the [docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find), which illustrates this further.\n",
    "\n",
    "Using `.find_all()`, we can expect to get back all of the matches (resorts, in this case) present on a given state's page (even if there is only one, as is sometimes the case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 'State name': Alaska\n",
      "Scraping 'Resorts': ['Alyeska', 'Eaglecrest', 'Majestic Heli Ski']\n",
      "Scraping 'Forecasts': [['0\"', '0\"', '0\"', '0\"', '0\"', '0\"', '0\"', '0\"', '0\"', '1-3\"'], ['8\"'], ['0-1\"', '0-1\"', '1-3\"', '0-1\"', '1-2\"', '1-2\"', '1-2\"', '0\"', '0\"', '0\"'], ['N/A'], ['4-6\"', '0-1\"', '1-3\"', '5-9\"', '7-11\"', '1-2\"', '0\"', '0\"', '0\"', '0-1\"'], ['N/A']]\n"
     ]
    }
   ],
   "source": [
    "# Select page region to use\n",
    "big_column = state_soup.find(class_='col-lg-8')\n",
    "\n",
    "# Get state name\n",
    "state_name = state_soup.find('h1', {'class': 'title'}).get_text()\n",
    "print(\"Scraping 'State name': {}\".format(state_name))\n",
    "\n",
    "# Get resort name(s)\n",
    "resorts_scrape = big_column.find_all('div', {'class': 'title-location'})\n",
    "resorts = [resort.getText() for resort in resorts_scrape]\n",
    "print(\"Scraping 'Resorts': {}\".format(resorts))\n",
    "\n",
    "# Scrape the table data\n",
    "table_scrape = big_column.find_all('table', {'class': 'tiny-graph'})\n",
    "    \n",
    "# Get forecast for each resort\n",
    "forecasts = []\n",
    "for table in table_scrape:\n",
    "    daily_snow = table.find_all('span')\n",
    "    forecast_n = [val['value'] for val in daily_snow if val.has_attr('value')]\n",
    "    forecasts.append(forecast_n)\n",
    "\n",
    "print(\"Scraping 'Forecasts': {}\".format(forecasts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we're in business!** Notice the correspondence between the forecast we see in the screenshot below and the data that we scraped? We've still got a bit of cleaning to do, but the data is in our hands!\n",
    "<img src='../assets/images/web_scraping/scrape_example.png' style=\"width: 500px;\"/>\n",
    "> To **re-cap**, we currently have:\n",
    "> - the name of each state\n",
    "> - the name of each resort in each state\n",
    "> - the forecast at each resort\n",
    "Looks like we are ready to move on to the next phase: using **google-maps** to collect location data for each resort we have scrapped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"google\"></a>\n",
    "## Part 2. Location Data with Google Maps API\n",
    "\n",
    "For this section, we'll use a [Python client library](https://github.com/googlemaps/google-maps-services-python) for Google Maps API Web Services to look-up the lat. and long. of each resort. This tool brings together multiple Google utilities including [directions](https://developers.google.com/maps/documentation/directions/), [distances](https://developers.google.com/maps/documentation/distancematrix/), [geocoding](https://developers.google.com/maps/documentation/geocoding/), [elevation](https://developers.google.com/maps/documentation/elevation/), and [more](https://developers.google.com/places/) and makes them available to your Python app. \n",
    "> To install:  `pip install -U googlemaps`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"key\"></a>\n",
    "### API Key\n",
    "\n",
    "Like most modern API's- each Google Maps request **requires an API key** to ID each client. API keys are freely available to anyone with a Google account at https://developers.google.com/console. The type of API key you need is a 'Server key'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google_geocode_url= 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\tparams_geocode = {'address': (ski_name+' '+ski_state),\n",
    "\t\t\t\t\t\t'key':secrets.google_places_key,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"study\"></a>\n",
    "#### Further Study\n",
    "- Al Sweigart, \"[Automating Your Browser and Desktop Apps](https://www.youtube.com/watch?v=dZLyfbSQPXI)\" (Quick survey of tools)\n",
    "- Asheesh Laroia, \"[Web scraping: Reliably and efficiently pull data from pages that don't expect it\n",
    "](https://www.youtube.com/watch?v=52wxGESwQSA)\" (In-depth, hands-on tutorial)\n",
    "- Data Science Dojo, \"[Intro to Web Scraping with Python and Beautiful Soup](https://www.youtube.com/watch?v=XQgXKtPSzUI)\" (Quick and dirty demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
